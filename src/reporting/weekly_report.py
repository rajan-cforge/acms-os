"""
Weekly Enterprise Report Generator (Week 7 Task 1)

Auto-generates executive-facing intelligence reports every Monday.
Shows leadership what their teams are struggling with - zero manual analysis.

Report Sections:
1. Executive Summary - Top patterns by cost impact
2. Productivity Blockers - Issues blocking teams
3. Knowledge Gaps - Repeated questions, documentation needs
4. Quality Issues - Bugs and testing gaps
5. Innovation Ideas - Team suggestions with ROI
6. Metrics Dashboard - Cache performance, cost savings
"""

from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, date
from dataclasses import dataclass, asdict
import asyncio
import json
import logging

logger = logging.getLogger(__name__)


@dataclass
class ReportMetrics:
    """Key metrics for the reporting period"""
    total_queries: int
    cache_hits: int
    cache_hit_rate: float
    cost_savings_usd: float
    patterns_detected: int
    total_impact_usd: float
    week_start: date
    week_end: date


@dataclass
class PatternSummary:
    """Summary of a detected pattern for reporting"""
    pattern_id: str
    category: str
    description: str
    mentions: int
    negative_feedback_rate: float
    monthly_impact_usd: float
    priority_score: float
    trend: str  # "increasing", "stable", "decreasing"
    recommendations: List[str]


@dataclass
class WeeklyReport:
    """Complete weekly enterprise report"""
    report_id: str
    generated_at: datetime
    week_start: date
    week_end: date

    # Executive Summary
    executive_summary: str
    top_patterns: List[PatternSummary]
    total_impact: float
    week_over_week_change: float

    # Detailed Sections
    productivity_blockers: List[PatternSummary]
    knowledge_gaps: List[PatternSummary]
    quality_issues: List[PatternSummary]
    innovation_ideas: List[PatternSummary]

    # Metrics
    metrics: ReportMetrics

    # Metadata
    patterns_analyzed: int
    memories_scanned: int


class WeeklyReportGenerator:
    """
    Generate weekly enterprise intelligence reports

    Automatically aggregates patterns, calculates impact, and generates
    executive-facing reports with actionable insights.
    """

    def __init__(self, db_pool):
        """
        Initialize report generator

        Args:
            db_pool: AsyncPG database connection pool
        """
        self.db_pool = db_pool

    async def generate_report(
        self,
        week_start: Optional[date] = None,
        week_end: Optional[date] = None
    ) -> WeeklyReport:
        """
        Generate comprehensive weekly report

        Args:
            week_start: Start of reporting period (default: last Monday)
            week_end: End of reporting period (default: last Sunday)

        Returns:
            WeeklyReport object with all sections populated

        Example:
            >>> generator = WeeklyReportGenerator(db_pool)
            >>> report = await generator.generate_report()
            >>> print(report.executive_summary)
            'Week of Oct 16-22: 6 critical patterns detected...'
        """
        # Default to last week (Monday to Sunday)
        if week_start is None or week_end is None:
            week_start, week_end = self._get_last_week_dates()

        logger.info(f"Generating weekly report for {week_start} to {week_end}")

        # Step 1: Fetch detected patterns for the period
        patterns = await self._fetch_patterns(week_start, week_end)

        # Step 2: Calculate metrics
        metrics = await self._calculate_metrics(week_start, week_end)

        # Step 3: Categorize patterns
        categorized = self._categorize_patterns(patterns)

        # Step 4: Calculate week-over-week change
        wow_change = await self._calculate_week_over_week(week_start)

        # Step 5: Generate executive summary
        exec_summary = self._generate_executive_summary(
            categorized['top_patterns'],
            metrics,
            wow_change
        )

        # Step 6: Create report
        report = WeeklyReport(
            report_id=f"report_{week_start.isoformat()}",
            generated_at=datetime.now(),
            week_start=week_start,
            week_end=week_end,
            executive_summary=exec_summary,
            top_patterns=categorized['top_patterns'][:5],
            total_impact=sum(p.monthly_impact_usd for p in patterns),
            week_over_week_change=wow_change,
            productivity_blockers=categorized['blockers'],
            knowledge_gaps=categorized['gaps'],
            quality_issues=categorized['quality'],
            innovation_ideas=categorized['ideas'],
            metrics=metrics,
            patterns_analyzed=len(patterns),
            memories_scanned=metrics.total_queries
        )

        # Step 7: Store report in database
        await self._store_report(report)

        logger.info(f"Report generated: {len(patterns)} patterns, ${metrics.total_impact_usd:.2f} impact")

        return report

    def _get_last_week_dates(self) -> tuple:
        """Calculate last Monday to Sunday"""
        today = date.today()
        # Find last Monday
        days_since_monday = (today.weekday() + 7) % 7
        if days_since_monday == 0:
            days_since_monday = 7  # If today is Monday, go back to last Monday
        last_monday = today - timedelta(days=days_since_monday)
        last_sunday = last_monday + timedelta(days=6)

        return last_monday, last_sunday

    async def _fetch_patterns(
        self,
        week_start: date,
        week_end: date
    ) -> List[PatternSummary]:
        """
        Fetch detected patterns for the period

        Note: In production, this would call the pattern detector.
        For now, returns patterns from the last detection run.
        """
        from src.intelligence.pattern_detector import PatternDetector

        detector = PatternDetector(self.db_pool)

        # Get patterns (lookback_days based on week range)
        lookback_days = (date.today() - week_start).days + 7
        raw_patterns = await detector.detect_patterns(
            lookback_days=lookback_days,
            min_mentions=3
        )

        # Convert to PatternSummary format
        patterns = []
        for p in raw_patterns:
            patterns.append(PatternSummary(
                pattern_id=p.pattern_id,
                category=p.category.value,
                description=p.description,
                mentions=p.mentions,
                negative_feedback_rate=p.negative_feedback_rate,
                monthly_impact_usd=self._calculate_monthly_impact(p),
                priority_score=p.priority_score,
                trend=self._classify_trend(p.trend_30day),
                recommendations=self._generate_recommendations(p)
            ))

        return patterns

    def _calculate_monthly_impact(self, pattern) -> float:
        """
        Calculate monthly cost impact for a pattern

        Formula:
        - Productivity Blocker: mentions √ó 2 hours √ó $100/hour
        - Knowledge Gap: mentions √ó 1 hour √ó $100/hour
        - Quality Issue: mentions √ó 2 hours √ó $100/hour + fix cost
        - Innovation Idea: -potential_roi (negative = savings)
        """
        hourly_rate = 100  # $100/hour developer time

        if pattern.category.value == "PRODUCTIVITY_BLOCKER":
            return pattern.mentions * 2 * hourly_rate
        elif pattern.category.value == "KNOWLEDGE_GAP":
            return pattern.mentions * 1 * hourly_rate
        elif pattern.category.value == "QUALITY_ISSUE":
            fix_cost = 2000  # Estimated fix cost
            return (pattern.mentions * 2 * hourly_rate) + fix_cost
        elif pattern.category.value == "INNOVATION_IDEA":
            # Potential savings (negative impact = good)
            return -(pattern.mentions * 0.5 * hourly_rate)
        else:
            return pattern.mentions * 1 * hourly_rate

    def _classify_trend(self, trend_value: float) -> str:
        """Classify trend as increasing/stable/decreasing"""
        if trend_value > 0.2:
            return "increasing"
        elif trend_value < -0.2:
            return "decreasing"
        else:
            return "stable"

    def _generate_recommendations(self, pattern) -> List[str]:
        """Generate actionable recommendations for a pattern"""
        category = pattern.category.value

        if category == "PRODUCTIVITY_BLOCKER":
            return [
                f"Prioritize fix in next sprint (impact: ${self._calculate_monthly_impact(pattern):,.0f}/month)",
                "Assign dedicated engineer to resolve blocker",
                "Set up monitoring to track resolution progress"
            ]
        elif category == "KNOWLEDGE_GAP":
            return [
                f"Create documentation for '{pattern.description}'",
                "Schedule knowledge-sharing session",
                f"Add FAQ section (saves ${self._calculate_monthly_impact(pattern):,.0f}/month)"
            ]
        elif category == "QUALITY_ISSUE":
            return [
                f"Fix bug in next sprint (ROI: ${self._calculate_monthly_impact(pattern):,.0f}/month)",
                "Add automated tests to prevent recurrence",
                "Review related code for similar issues"
            ]
        elif category == "INNOVATION_IDEA":
            return [
                f"Evaluate feasibility (potential ROI: ${-self._calculate_monthly_impact(pattern):,.0f}/month)",
                "Create proof-of-concept",
                "Present to product team for prioritization"
            ]
        else:
            return ["Review and prioritize for next planning cycle"]

    async def _calculate_metrics(
        self,
        week_start: date,
        week_end: date
    ) -> ReportMetrics:
        """Calculate key metrics for the reporting period"""
        async with self.db_pool.acquire() as conn:
            # Use memory_items as proxy for query count (each memory = interaction)
            query_count = await conn.fetchval("""
                SELECT COUNT(*)
                FROM memory_items
                WHERE created_at >= $1 AND created_at < $2
            """, week_start, week_end + timedelta(days=1))

            # For cache metrics, use semantic_cache table if exists
            try:
                cache_hits = await conn.fetchval("""
                    SELECT COUNT(*)
                    FROM semantic_cache
                    WHERE created_at >= $1 AND created_at < $2
                """, week_start, week_end + timedelta(days=1))
            except Exception:
                # If semantic_cache doesn't exist or has different schema, estimate
                cache_hits = int(query_count * 0.3)  # Assume 30% hit rate

            # Calculate cache hit rate
            cache_hit_rate = (cache_hits / query_count * 100) if query_count > 0 else 0

            # Cost savings (assumes $0.0221 per cache hit)
            cost_savings = cache_hits * 0.0221

            # Patterns detected (from pattern detector)
            from src.intelligence.pattern_detector import PatternDetector
            detector = PatternDetector(self.db_pool)
            lookback_days = (date.today() - week_start).days + 7
            patterns = await detector.detect_patterns(lookback_days=lookback_days, min_mentions=3)

            total_impact = sum(self._calculate_monthly_impact(p) for p in patterns)

            return ReportMetrics(
                total_queries=query_count or 0,
                cache_hits=cache_hits or 0,
                cache_hit_rate=cache_hit_rate,
                cost_savings_usd=cost_savings,
                patterns_detected=len(patterns),
                total_impact_usd=total_impact,
                week_start=week_start,
                week_end=week_end
            )

    def _categorize_patterns(
        self,
        patterns: List[PatternSummary]
    ) -> Dict[str, List[PatternSummary]]:
        """Categorize patterns by type"""
        blockers = [p for p in patterns if p.category == "PRODUCTIVITY_BLOCKER"]
        gaps = [p for p in patterns if p.category == "KNOWLEDGE_GAP"]
        quality = [p for p in patterns if p.category == "QUALITY_ISSUE"]
        ideas = [p for p in patterns if p.category == "INNOVATION_IDEA"]

        # Sort each by priority
        blockers.sort(key=lambda p: p.priority_score, reverse=True)
        gaps.sort(key=lambda p: p.priority_score, reverse=True)
        quality.sort(key=lambda p: p.priority_score, reverse=True)
        ideas.sort(key=lambda p: p.priority_score, reverse=True)

        # Top patterns overall
        top_patterns = sorted(patterns, key=lambda p: p.monthly_impact_usd, reverse=True)

        return {
            'blockers': blockers,
            'gaps': gaps,
            'quality': quality,
            'ideas': ideas,
            'top_patterns': top_patterns
        }

    async def _calculate_week_over_week(self, current_week_start: date) -> float:
        """
        Calculate week-over-week change in total impact

        Returns percentage change (e.g., 0.15 = 15% increase)
        """
        # Get previous week
        prev_week_start = current_week_start - timedelta(days=7)
        prev_week_end = prev_week_start + timedelta(days=6)

        # Calculate previous week's impact
        try:
            prev_patterns = await self._fetch_patterns(prev_week_start, prev_week_end)
            prev_impact = sum(p.monthly_impact_usd for p in prev_patterns)

            curr_patterns = await self._fetch_patterns(current_week_start, current_week_start + timedelta(days=6))
            curr_impact = sum(p.monthly_impact_usd for p in curr_patterns)

            if prev_impact > 0:
                return (curr_impact - prev_impact) / prev_impact
            else:
                return 0.0
        except Exception as e:
            logger.warning(f"Could not calculate WoW change: {e}")
            return 0.0

    def _generate_executive_summary(
        self,
        top_patterns: List[PatternSummary],
        metrics: ReportMetrics,
        wow_change: float
    ) -> str:
        """Generate executive summary text"""
        week_str = f"{metrics.week_start.strftime('%b %d')}-{metrics.week_end.strftime('%d, %Y')}"

        summary_parts = []

        # Header
        summary_parts.append(f"**Week of {week_str}**")
        summary_parts.append("")

        # Key findings
        summary_parts.append(f"**Key Findings:**")
        summary_parts.append(f"- {metrics.patterns_detected} organizational patterns detected")
        summary_parts.append(f"- ${metrics.total_impact_usd:,.0f}/month in estimated productivity impact")

        wow_str = f"{abs(wow_change)*100:.0f}%"
        if wow_change > 0.1:
            summary_parts.append(f"- ‚ö†Ô∏è Impact increased {wow_str} from last week")
        elif wow_change < -0.1:
            summary_parts.append(f"- ‚úÖ Impact decreased {wow_str} from last week")

        summary_parts.append("")

        # Top 3 patterns
        summary_parts.append("**Top 3 Priorities:**")
        for i, pattern in enumerate(top_patterns[:3], 1):
            emoji = "üî¥" if pattern.category == "PRODUCTIVITY_BLOCKER" else "üü°"
            summary_parts.append(
                f"{i}. {emoji} {pattern.description} "
                f"({pattern.mentions} mentions, ${pattern.monthly_impact_usd:,.0f}/month)"
            )

        summary_parts.append("")

        # Metrics
        summary_parts.append("**Performance:**")
        summary_parts.append(f"- Cache hit rate: {metrics.cache_hit_rate:.1f}%")
        summary_parts.append(f"- Cost savings: ${metrics.cost_savings_usd:.2f}")
        summary_parts.append(f"- Queries handled: {metrics.total_queries:,}")

        return "\n".join(summary_parts)

    async def _store_report(self, report: WeeklyReport):
        """Store report in database"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO enterprise_reports (
                    report_id,
                    generated_at,
                    week_start,
                    week_end,
                    executive_summary,
                    report_data,
                    total_impact_usd,
                    patterns_detected
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                ON CONFLICT (report_id) DO UPDATE SET
                    generated_at = EXCLUDED.generated_at,
                    report_data = EXCLUDED.report_data,
                    total_impact_usd = EXCLUDED.total_impact_usd
            """,
                report.report_id,
                report.generated_at,
                report.week_start,
                report.week_end,
                report.executive_summary,
                json.dumps(asdict(report), default=str),
                report.total_impact,
                report.patterns_analyzed
            )

        logger.info(f"Stored report {report.report_id} in database")

    def to_html(self, report: WeeklyReport) -> str:
        """
        Convert report to HTML email format

        Returns HTML string ready for email delivery
        """
        from src.reporting.email_template import generate_html_email
        return generate_html_email(report)

    def to_text(self, report: WeeklyReport) -> str:
        """
        Convert report to plain text format

        Returns plain text string for non-HTML email clients
        """
        from src.reporting.email_template import generate_text_email
        return generate_text_email(report)

    def to_dict(self, report: WeeklyReport) -> Dict[str, Any]:
        """Convert report to dictionary for API responses"""
        return asdict(report)


# Convenience function
async def generate_weekly_report(db_pool, week_start: Optional[date] = None) -> WeeklyReport:
    """
    Generate weekly report (convenience function)

    Args:
        db_pool: Database connection pool
        week_start: Start of week (default: last Monday)

    Returns:
        WeeklyReport object
    """
    generator = WeeklyReportGenerator(db_pool)
    return await generator.generate_report(week_start=week_start)
